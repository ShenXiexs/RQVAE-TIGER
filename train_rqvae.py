# train_rqvae.py

import gin
import os
import json
import re
import torch
import numpy as np
import wandb
import matplotlib.pyplot as plt
from scipy.stats import entropy
from collections import Counter, defaultdict
from typing import Dict, List, Tuple

from accelerate import Accelerator
from torch.optim import AdamW
from torch.utils.data import DataLoader
from tqdm import tqdm

# æ•°æ®åŠ è½½å™¨
from data.custom_parquet_vectors import ParquetVectorIterable
from modules.rqvae import RqVae
from modules.quantize import QuantizeForwardMode
from modules.utils import parse_config

torch._dynamo.disable()


class RQVAEEvaluator:
    """ä¸“é—¨ç”¨äºRQ-VAEç¬¬ä¸€é˜¶æ®µè¯„ä¼°çš„å·¥å…·ç±»"""
    
    def __init__(self, n_layers: int, codebook_size: int):
        self.n_layers = n_layers
        self.codebook_size = codebook_size
        self.history = {
            'steps': [],
            'codebook_usage': [[] for _ in range(n_layers)],
            'dead_codes': [[] for _ in range(n_layers)], 
            'perplexity': [[] for _ in range(n_layers)],
            'gini_coefficient': [[] for _ in range(n_layers)],
            'collision_rate': [],
            'max_bucket_size': [],
            'eval_recon': [],
            'eval_total': []
        }
    
    @torch.no_grad()
    def evaluate_model(self, model, eval_dataloader, max_samples=10000, device='cuda') -> Dict:
        """å®Œæ•´è¯„ä¼°RQ-VAEæ¨¡å‹ï¼Œè¿”å›æ‰€æœ‰å…³é”®æŒ‡æ ‡"""
        model.eval()
        
        all_sem_ids = []
        eval_losses = []
        eval_recon_losses = []
        samples_processed = 0
        
        print(f"å¼€å§‹è¯„ä¼°æ¨¡å‹... æœ€å¤šå¤„ç† {max_samples} ä¸ªæ ·æœ¬")
        
        eval_iter = iter(eval_dataloader)
        with tqdm(total=min(max_samples, 50000), desc="è¯„ä¼°ä¸­", leave=False) as pbar:
            while samples_processed < max_samples:
                try:
                    batch = next(eval_iter)
                except StopIteration:
                    break
                
                # è·å–semantic IDs
                qout = model.get_semantic_ids(batch, gumbel_t=0.001)
                all_sem_ids.append(qout.sem_ids.cpu())  # [n_layers, batch]
                
                # è®¡ç®—æŸå¤±
                out = model(batch, gumbel_t=0.001)
                eval_losses.append(out.loss.item())
                eval_recon_losses.append(out.reconstruction_loss.item())
                
                batch_size = batch.shape[0]
                samples_processed += batch_size
                pbar.update(batch_size)
                
                # å¤§æ•°æ®é›†ä¸‹é™åˆ¶å†…å­˜ä½¿ç”¨
                if len(all_sem_ids) > 100:  # é¿å…å†…å­˜çˆ†ç‚¸
                    break
        
        if not all_sem_ids:
            print("âŒ è¯„ä¼°å¤±è´¥ï¼šæ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")
            return self._get_empty_metrics()
        
        # åˆå¹¶æ‰€æœ‰semantic IDs: [n_layers, total_samples]
        all_sem_ids = torch.cat(all_sem_ids, dim=1)
        
        metrics = {
            'eval_loss': np.mean(eval_losses),
            'eval_recon': np.mean(eval_recon_losses),
            'samples_evaluated': samples_processed
        }
        
        # è®¡ç®—å„å±‚æŒ‡æ ‡
        for layer_idx in range(self.n_layers):
            layer_ids = all_sem_ids[layer_idx].numpy()
            layer_metrics = self._compute_layer_metrics(layer_ids, layer_idx)
            metrics.update(layer_metrics)
        
        # è®¡ç®—ç¢°æ’æŒ‡æ ‡
        collision_metrics = self._compute_collision_metrics(all_sem_ids)
        metrics.update(collision_metrics)
        
        return metrics
    
    def _compute_layer_metrics(self, layer_ids: np.ndarray, layer_idx: int) -> Dict:
        """è®¡ç®—å•å±‚çš„æ‰€æœ‰æŒ‡æ ‡"""
        unique_ids, counts = np.unique(layer_ids, return_counts=True)
        
        # 1. ç æœ¬ä½¿ç”¨ç‡
        used_codes = len(unique_ids)
        usage_rate = used_codes / self.codebook_size
        
        # 2. æ­»äº¡ç æ¯”ä¾‹
        dead_codes = self.codebook_size - used_codes
        dead_code_rate = dead_codes / self.codebook_size
        
        # 3. å›°æƒ‘åº¦
        probs = counts / counts.sum()
        H = entropy(probs, base=np.e)  # è‡ªç„¶å¯¹æ•°
        perplexity = np.exp(H)
        
        # 4. Giniç³»æ•° (è¡¡é‡åˆ†å¸ƒä¸å‡åŒ€åº¦)
        gini = self._compute_gini_coefficient(counts)
        
        return {
            f'layer_{layer_idx}_usage_rate': usage_rate,
            f'layer_{layer_idx}_used_codes': used_codes,
            f'layer_{layer_idx}_dead_codes': dead_codes,
            f'layer_{layer_idx}_dead_code_rate': dead_code_rate,
            f'layer_{layer_idx}_perplexity': perplexity,
            f'layer_{layer_idx}_entropy': H,
            f'layer_{layer_idx}_gini': gini,
            f'layer_{layer_idx}_max_count': counts.max(),
            f'layer_{layer_idx}_min_count': counts.min() if len(counts) > 0 else 0
        }
    
    def _compute_collision_metrics(self, all_sem_ids: torch.Tensor) -> Dict:
        """è®¡ç®—ç¢°æ’æŒ‡æ ‡ - å‰3å±‚prefixçš„é‡å¤æƒ…å†µ"""
        # å–å‰n-1å±‚ä½œä¸ºprefixï¼Œç»Ÿè®¡ç¢°æ’
        prefix_layers = min(self.n_layers - 1, 3)  # æœ€å¤šå–å‰3å±‚
        if prefix_layers <= 0:
            return {'collision_rate': 0.0, 'max_bucket_size': 1}
        
        # æ„å»ºprefix tuples (é‡‡æ ·ä»¥é¿å…å†…å­˜é—®é¢˜)
        total_samples = all_sem_ids.shape[1]
        sample_size = min(total_samples, 10000)  # æœ€å¤šé‡‡æ ·1ä¸‡ä¸ª
        indices = torch.randperm(total_samples)[:sample_size]
        
        prefixes = []
        for i in indices:
            prefix = tuple(all_sem_ids[:prefix_layers, i].tolist())
            prefixes.append(prefix)
        
        # ç»Ÿè®¡é‡å¤
        prefix_counts = Counter(prefixes)
        
        # ç¢°æ’ç‡ï¼šæœ‰é‡å¤çš„prefixå æ€»unique prefixçš„æ¯”ä¾‹
        collision_buckets = sum(1 for count in prefix_counts.values() if count > 1)
        collision_rate = collision_buckets / len(prefix_counts) if len(prefix_counts) > 0 else 0.0
        
        # æœ€å¤§æ¡¶å¤§å°
        max_bucket_size = max(prefix_counts.values()) if prefix_counts else 1
        
        return {
            'collision_rate': collision_rate,
            'max_bucket_size': max_bucket_size,
            'unique_prefixes': len(prefix_counts),
            'sampled_for_collision': len(prefixes)
        }
    
    def _compute_gini_coefficient(self, counts: np.ndarray) -> float:
        """è®¡ç®—Giniç³»æ•°ï¼Œè¡¡é‡åˆ†å¸ƒä¸å‡åŒ€ç¨‹åº¦"""
        if len(counts) == 0:
            return 1.0
        
        counts = np.sort(counts)
        n = len(counts)
        cumsum = np.cumsum(counts)
        return 1 - 2 * np.sum(cumsum) / (n * cumsum[-1]) + 1 / n
    
    def _get_empty_metrics(self) -> Dict:
        """è¿”å›ç©ºçš„é»˜è®¤æŒ‡æ ‡"""
        metrics = {
            'eval_loss': 999.0,
            'eval_recon': 999.0,
            'samples_evaluated': 0,
            'collision_rate': 1.0,
            'max_bucket_size': 999,
            'unique_prefixes': 0,
            'sampled_for_collision': 0
        }
        
        for layer_idx in range(self.n_layers):
            metrics.update({
                f'layer_{layer_idx}_usage_rate': 0.0,
                f'layer_{layer_idx}_used_codes': 0,
                f'layer_{layer_idx}_dead_codes': self.codebook_size,
                f'layer_{layer_idx}_dead_code_rate': 1.0,
                f'layer_{layer_idx}_perplexity': 1.0,
                f'layer_{layer_idx}_entropy': 0.0,
                f'layer_{layer_idx}_gini': 1.0,
                f'layer_{layer_idx}_max_count': 0,
                f'layer_{layer_idx}_min_count': 0
            })
        
        return metrics
    
    def update_history(self, step: int, metrics: Dict):
        """æ›´æ–°å†å²è®°å½•"""
        self.history['steps'].append(step)
        self.history['eval_recon'].append(metrics['eval_recon'])
        self.history['eval_total'].append(metrics['eval_loss'])
        self.history['collision_rate'].append(metrics['collision_rate'])
        self.history['max_bucket_size'].append(metrics['max_bucket_size'])
        
        for layer_idx in range(self.n_layers):
            self.history['codebook_usage'][layer_idx].append(
                metrics[f'layer_{layer_idx}_usage_rate']
            )
            self.history['dead_codes'][layer_idx].append(
                metrics[f'layer_{layer_idx}_dead_code_rate']
            )
            self.history['perplexity'][layer_idx].append(
                metrics[f'layer_{layer_idx}_perplexity']
            )
            self.history['gini_coefficient'][layer_idx].append(
                metrics[f'layer_{layer_idx}_gini']
            )
    
    def check_convergence(self, min_steps=15000, window_size=5, 
                         usage_threshold=0.75, dead_code_threshold=0.08,  # æ”¾å®½ä¸€äº›é˜ˆå€¼
                         stability_threshold=0.01) -> Tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦æ”¶æ•›ï¼ˆé’ˆå¯¹å¤§è§„æ¨¡æ•°æ®é€‚å½“æ”¾å®½æ ‡å‡†ï¼‰"""
        if len(self.history['steps']) < window_size:
            return False, f"è¯„ä¼°æ¬¡æ•°ä¸è¶³ï¼Œéœ€è¦è‡³å°‘{window_size}æ¬¡"
        
        current_step = self.history['steps'][-1]
        if current_step < min_steps:
            return False, f"è®­ç»ƒæ­¥æ•°ä¸è¶³ï¼Œå½“å‰{current_step}ï¼Œéœ€è¦è‡³å°‘{min_steps}"
        
        # æ£€æŸ¥æœ€è¿‘window_sizeæ¬¡çš„ç¨³å®šæ€§
        recent_recon = self.history['eval_recon'][-window_size:]
        recon_cv = np.std(recent_recon) / np.mean(recent_recon)  # å˜å¼‚ç³»æ•°
        
        if recon_cv > stability_threshold:
            return False, f"é‡æ„æŸå¤±ä¸ç¨³å®šï¼ŒCV={recon_cv:.4f} > {stability_threshold}"
        
        # æ£€æŸ¥å„å±‚ç æœ¬ä½¿ç”¨ç‡ï¼ˆæ”¾å®½æ ‡å‡†ï¼‰
        for layer_idx in range(self.n_layers):
            recent_usage = self.history['codebook_usage'][layer_idx][-window_size:]
            recent_dead = self.history['dead_codes'][layer_idx][-window_size:]
            
            avg_usage = np.mean(recent_usage)
            avg_dead = np.mean(recent_dead)
            usage_var = np.var(recent_usage)
            
            if avg_usage < usage_threshold:
                return False, f"Layer {layer_idx} ä½¿ç”¨ç‡ä¸è¶³: {avg_usage:.3f} < {usage_threshold}"
            
            if avg_dead > dead_code_threshold:
                return False, f"Layer {layer_idx} æ­»äº¡ç è¿‡å¤š: {avg_dead:.3f} > {dead_code_threshold}"
            
            if usage_var > 0.02:  # ç¨å¾®æ”¾å®½ä½¿ç”¨ç‡æ–¹å·®é˜ˆå€¼ 
                return False, f"Layer {layer_idx} ä½¿ç”¨ç‡ä¸ç¨³å®š: var={usage_var:.4f}"
        
        return True, "æ”¶æ•›æ¡ä»¶æ»¡è¶³ï¼"
    
    def should_save_as_best(self, current_metrics: Dict, best_metrics: Dict = None) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿å­˜ä¸ºæœ€ä½³æ¨¡å‹"""
        if best_metrics is None:
            return True
        
        current_recon = current_metrics['eval_recon']
        best_recon = best_metrics['eval_recon']
        
        # ä¸»è¦æŒ‡æ ‡ï¼šé‡æ„æŸå¤±æ›´ä½
        if current_recon < best_recon * 0.999:  # è‡³å°‘æ”¹å–„0.1%
            return True
        
        # ç›¸è¿‘æ—¶çš„tie-breakerï¼šæ›´é«˜çš„å¹³å‡ä½¿ç”¨ç‡
        if abs(current_recon - best_recon) / best_recon < 0.001:  # å·®å¼‚<0.1%
            current_avg_usage = np.mean([
                current_metrics[f'layer_{i}_usage_rate'] for i in range(self.n_layers)
            ])
            best_avg_usage = np.mean([
                best_metrics[f'layer_{i}_usage_rate'] for i in range(self.n_layers)
            ])
            return current_avg_usage > best_avg_usage
        
        return False
    
    def save_evaluation_summary(self, save_dir: str, metrics: Dict):
        """ä¿å­˜è¯„ä¼°æ‘˜è¦"""
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜æ•°å€¼æŒ‡æ ‡
        clean_metrics = {}
        for k, v in metrics.items():
            if isinstance(v, (np.integer, np.floating)):
                clean_metrics[k] = float(v)
            else:
                clean_metrics[k] = v
        
        with open(os.path.join(save_dir, 'evaluation_metrics.json'), 'w') as f:
            json.dump(clean_metrics, f, indent=2)
        
        # ç”Ÿæˆç®€åŒ–çš„å¯è§†åŒ–
        self._plot_simple_summary(save_dir, metrics)
        
        print(f"âœ“ è¯„ä¼°æ‘˜è¦å·²ä¿å­˜åˆ°: {save_dir}")
    
    def _plot_simple_summary(self, save_dir: str, metrics: Dict):
        """ç”Ÿæˆç®€åŒ–çš„è¯„ä¼°å›¾è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        layer_indices = list(range(self.n_layers))
        
        # 1. å„å±‚ä½¿ç”¨ç‡
        ax = axes[0, 0]
        usage_rates = [metrics[f'layer_{i}_usage_rate'] for i in range(self.n_layers)]
        bars = ax.bar(layer_indices, usage_rates, color='skyblue', alpha=0.7)
        ax.axhline(y=0.75, color='red', linestyle='--', alpha=0.7, label='ç›®æ ‡é˜ˆå€¼')
        ax.set_xlabel('Layer')
        ax.set_ylabel('Usage Rate')
        ax.set_title('å„å±‚ç æœ¬ä½¿ç”¨ç‡')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 2. å„å±‚å›°æƒ‘åº¦
        ax = axes[0, 1]
        perplexities = [metrics[f'layer_{i}_perplexity'] for i in range(self.n_layers)]
        ax.bar(layer_indices, perplexities, color='lightgreen', alpha=0.7)
        ax.set_xlabel('Layer')
        ax.set_ylabel('Perplexity')
        ax.set_title('å„å±‚å›°æƒ‘åº¦')
        ax.grid(True, alpha=0.3)
        
        # 3. æ­»äº¡ç æ¯”ä¾‹
        ax = axes[1, 0]
        dead_rates = [metrics[f'layer_{i}_dead_code_rate'] for i in range(self.n_layers)]
        bars = ax.bar(layer_indices, dead_rates, color='salmon', alpha=0.7)
        ax.axhline(y=0.08, color='red', linestyle='--', alpha=0.7, label='è­¦æˆ’çº¿')
        ax.set_xlabel('Layer')
        ax.set_ylabel('Dead Code Rate')
        ax.set_title('å„å±‚æ­»äº¡ç æ¯”ä¾‹')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 4. å…³é”®æŒ‡æ ‡æ‘˜è¦
        ax = axes[1, 1]
        ax.axis('off')
        avg_usage = np.mean(usage_rates)
        avg_dead = np.mean(dead_rates)
        avg_ppl = np.mean(perplexities)
        
        summary_text = f"""
å…³é”®æŒ‡æ ‡æ‘˜è¦:

é‡æ„æŸå¤±: {metrics['eval_recon']:.6f}
æ€»æŸå¤±: {metrics['eval_loss']:.6f}

å¹³å‡ä½¿ç”¨ç‡: {avg_usage:.3f}
å¹³å‡æ­»äº¡ç‡: {avg_dead:.3f}
å¹³å‡å›°æƒ‘åº¦: {avg_ppl:.1f}

ç¢°æ’ç‡: {metrics['collision_rate']:.3f}
æœ€å¤§æ¡¶å¤§å°: {metrics['max_bucket_size']}

è¯„ä¼°æ ·æœ¬æ•°: {metrics['samples_evaluated']:,}
        """
        ax.text(0.1, 0.9, summary_text, transform=ax.transAxes, 
               fontsize=10, verticalalignment='top', fontfamily='monospace',
               bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'evaluation_summary.png'), 
                   dpi=150, bbox_inches='tight')  # é™ä½DPIèŠ‚çœç©ºé—´
        plt.close()


@gin.configurable
def train(
    # ==== è®­ç»ƒè¶…å‚ ====
    iterations: int = 40000,
    batch_size: int = 1024,  # é™ä½æ‰¹é‡å¤§å°
    learning_rate: float = 5e-4,
    weight_decay: float = 0.01,
    gradient_accumulate_every: int = 2,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
    mixed_precision_type: str = "fp16",
    amp: bool = True,
    split_batches: bool = True,
    wandb_logging: bool = False,

    # ==== è·¯å¾„ ====
    save_dir_root: str = "trained_models/rqvae_xieshen/",
    pretrained_rqvae_path: str = None,

    # ==== æ•°æ® ====
    data_dirs=(
        "/mnt/xieshen/data/20250706",
        "/mnt/xieshen/data/20250707",
        "/mnt/xieshen/data/20250708",
        "/mnt/xieshen/data/20250709",
        "/mnt/xieshen/data/20250710",
        "/mnt/xieshen/data/20250711",
        "/mnt/xieshen/data/20250712",
    ),
    num_workers: int = 4,  # å¢åŠ æ•°æ®åŠ è½½å¹¶è¡Œåº¦
    input_normalize: str = "l2",

    # ==== ä¿å­˜/è¯„ä¼°é¢‘ç‡ ====
    save_model_every: int = 10000,
    do_eval: bool = True,
    eval_every: int = 3000,  # é€‚å½“é™ä½è¯„ä¼°é¢‘ç‡
    eval_samples: int = 8000,  # å‡å°‘è¯„ä¼°æ ·æœ¬æ•°

    # ==== RQ-VAE æ¨¡å‹å‚æ•° ====
    commitment_weight: float = 0.25,  # ä»è®ºæ–‡æ¨èå¼€å§‹
    vae_input_dim: int = 5120,
    vae_embed_dim: int = 512,
    vae_hidden_dims=[2048, 1024, 512],
    vae_codebook_size: int = 512,
    vae_codebook_normalize: bool = True,
    vae_codebook_mode=QuantizeForwardMode.ROTATION_TRICK,
    vae_sim_vq: bool = False,
    vae_n_layers: int = 3,
    vae_n_cat_feats: int = 0,
    vae_codebook_kmeans_init: bool = True,

    # ==== æ”¶æ•›åˆ¤æ®ï¼ˆé’ˆå¯¹å¤§æ•°æ®é›†é€‚å½“æ”¾å®½ï¼‰====
    convergence_min_steps: int = 15000,
    convergence_window_size: int = 5,
    convergence_usage_threshold: float = 0.75,  # ä»75%å¼€å§‹ï¼Œå¯ä»¥æ¥å—
    convergence_dead_threshold: float = 0.08,   # 8%æ­»äº¡ç å¯ä»¥æ¥å—
    convergence_stability_threshold: float = 0.01,  # 1%å˜å¼‚ç³»æ•°
):
    if wandb_logging:
        params = locals()

    # ==== åˆå§‹åŒ–è¯„ä¼°å™¨ ====
    evaluator = RQVAEEvaluator(
        n_layers=vae_n_layers,
        codebook_size=vae_codebook_size
    )
    
    print(f"è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    print(f"æ”¶æ•›æ¡ä»¶: æœ€å°‘{convergence_min_steps}æ­¥ï¼Œä½¿ç”¨ç‡â‰¥{convergence_usage_threshold*100}%ï¼Œæ­»äº¡ç â‰¤{convergence_dead_threshold*100}%")
    print(f"æ•°æ®è§„æ¨¡: 2åƒä¸‡æ¡è®°å½•ï¼Œ{vae_input_dim}ç»´å‘é‡")

    # ==== æŒ‡æ ‡è®°å½•ç»“æ„ ====
    metrics_history = {
        'steps': [], 'total_loss': [], 'recon_loss': [], 'vq_loss': [],
        'layer_usage': [[] for _ in range(vae_n_layers)],
        'layer_perplexity': [[] for _ in range(vae_n_layers)],
        'eval_loss': [], 'eval_recon': [], 'eval_vq': [], 'eval_steps': []
    }

    def ensure_list_lengths(metrics: dict):
        """ç¡®ä¿ layer_* çš„æ¯å±‚åˆ—è¡¨ä¸ steps å¯¹é½ã€‚"""
        target_len = len(metrics['steps'])
        for key in ['layer_usage', 'layer_perplexity']:
            for i, lst in enumerate(metrics[key]):
                while len(lst) < target_len:
                    lst.append(lst[-1] if lst else 0)

    def save_training_visualizations(metrics, save_dir, config):
        """ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–å›¾è¡¨å¹¶ä¿å­˜ JSON æŒ‡æ ‡ã€‚"""
        os.makedirs(save_dir, exist_ok=True)

        plt.style.use('default')
        plt.rcParams['font.size'] = 10
        plt.rcParams['figure.facecolor'] = 'white'

        fig = plt.figure(figsize=(16, 12))

        # 1. æŸå¤±æ¼”åŒ–
        ax1 = plt.subplot(3, 3, 1)
        plt.plot(metrics['steps'], metrics['total_loss'], 'r-', label='Total Loss', linewidth=2)
        plt.plot(metrics['steps'], metrics['recon_loss'], 'b-', label='Recon Loss', linewidth=2)
        plt.xlabel('Steps')
        plt.ylabel('Loss')
        plt.title('Loss Evolution')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # 2. VQ Loss
        ax2 = plt.subplot(3, 3, 2)
        plt.plot(metrics['steps'], metrics['vq_loss'], 'g-', label='VQ Loss', linewidth=2)
        plt.xlabel('Steps')
        plt.ylabel('VQ Loss')
        plt.title('Quantization Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # 3. ç æœ¬åˆ©ç”¨ç‡
        ax3 = plt.subplot(3, 3, 3)
        colors = ['orange', 'purple', 'cyan', 'teal', 'olive', 'brown']
        for i in range(len(metrics['layer_usage'])):
            if metrics['layer_usage'][i]:
                steps_usage = metrics['steps'][:len(metrics['layer_usage'][i])]
                plt.plot(steps_usage, metrics['layer_usage'][i],
                         color=colors[i % len(colors)], label=f'Layer {i}', linewidth=2)
        plt.xlabel('Steps')
        plt.ylabel('Used Codebook Entries')
        plt.title('Codebook Usage per Layer')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # 4. å›°æƒ‘åº¦æ¼”åŒ–
        ax4 = plt.subplot(3, 3, 4)
        for i in range(len(metrics['layer_perplexity'])):
            if metrics['layer_perplexity'][i]:
                steps_ppl = metrics['steps'][:len(metrics['layer_perplexity'][i])]
                plt.plot(steps_ppl, metrics['layer_perplexity'][i],
                         color=colors[i % len(colors)], label=f'Layer {i} PPL', linewidth=2)
        plt.xlabel('Steps')
        plt.ylabel('Perplexity')
        plt.title('Perplexity Evolution')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # 5. è¯„ä¼°æŒ‡æ ‡
        if metrics['eval_steps']:
            ax5 = plt.subplot(3, 3, 5)
            plt.plot(metrics['eval_steps'], metrics['eval_loss'], 'r--', label='Eval Total', marker='o')
            plt.plot(metrics['eval_steps'], metrics['eval_recon'], 'b--', label='Eval Recon', marker='s')
            plt.xlabel('Steps')
            plt.ylabel('Evaluation Loss')
            plt.title('Evaluation Metrics')
            plt.legend()
            plt.grid(True, alpha=0.3)

        plt.tight_layout()

        viz_path = os.path.join(save_dir, 'training_visualization.png')
        plt.savefig(viz_path, dpi=200, bbox_inches='tight', facecolor='white')  # é™ä½DPI
        plt.close()

        metrics_path = os.path.join(save_dir, 'training_metrics.json')
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2)

        print(f"âœ“ è®­ç»ƒå¯è§†åŒ–ä¿å­˜è‡³: {viz_path}")
        print(f"âœ“ è®­ç»ƒæ•°æ®ä¿å­˜è‡³: {metrics_path}")

    # ===== Accelerator =====
    accelerator = Accelerator(
        split_batches=split_batches,
        mixed_precision=mixed_precision_type if amp else "no",
    )
    device = accelerator.device

    print("=" * 60)
    print("RQ-VAEè®­ç»ƒ - å¤§è§„æ¨¡æ•°æ®ä¼˜åŒ–æ¨¡å¼")
    print("=" * 60)
    print(f"è®¾å¤‡: {device}")
    print(f"æ‰¹å¤§å°: {batch_size} (æ¢¯åº¦ç´¯ç§¯: {gradient_accumulate_every})")
    print(f"æœ‰æ•ˆæ‰¹å¤§å°: {batch_size * gradient_accumulate_every}")
    print(f"å­¦ä¹ ç‡: {learning_rate}")

    # ===== è®­ç»ƒæ•°æ®ï¼ˆè‡ªç„¶åˆ†å¸ƒï¼‰=====
    train_iterable = ParquetVectorIterable(
        data_dirs=list(data_dirs),
        column_vector="parsed_vector",
        column_id="md5_oaid",
        expected_dim=vae_input_dim,
        shuffle_files=True,
        shuffle_rows=False,
        seed=20250809,
        normalize=input_normalize,
        dtype="float32",
    )

    # Collateå‡½æ•°
    def collate(batch):
        # ParquetVectorIterable: yield (oid, tensor)ï¼Œè¿™é‡Œåªå–å¼ é‡åšè®­ç»ƒ
        _, vecs = zip(*batch)
        return torch.stack(vecs, dim=0)

    train_dataloader = DataLoader(
        train_iterable,
        batch_size=batch_size,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False,
        collate_fn=collate,
        prefetch_factor=2 if num_workers > 0 else None,  # é¢„å–ä¼˜åŒ–
    )

    # ===== è¯„ä¼°æ•°æ®ï¼ˆå°‘é‡ç”¨äºéªŒè¯ï¼‰ =====
    eval_dataloader = None
    if do_eval:
        eval_iterable = ParquetVectorIterable(
            data_dirs=[data_dirs[-1]],  # åªç”¨æœ€åä¸€ä¸ªç›®å½•åšè¯„ä¼°
            column_vector="parsed_vector",
            column_id="md5_oaid",
            expected_dim=vae_input_dim,
            shuffle_files=False,
            shuffle_rows=True,  # è¯„ä¼°æ—¶å¯ä»¥éšæœºé‡‡æ ·
            seed=20250809,
            normalize=input_normalize,
            dtype="float32",
        )
        eval_dataloader = DataLoader(
            eval_iterable,
            batch_size=batch_size,
            num_workers=min(2, num_workers),  # è¯„ä¼°ç”¨æ›´å°‘worker
            collate_fn=collate,
            pin_memory=True,
        )

    # ===== åˆ›å»ºæ¨¡å‹ =====
    model = RqVae(
        input_dim=vae_input_dim,
        embed_dim=vae_embed_dim,
        hidden_dims=list(vae_hidden_dims),
        codebook_size=vae_codebook_size,
        codebook_kmeans_init=vae_codebook_kmeans_init,
        codebook_normalize=vae_codebook_normalize,
        codebook_sim_vq=vae_sim_vq,
        codebook_mode=vae_codebook_mode,
        n_layers=vae_n_layers,
        n_cat_features=vae_n_cat_feats,
        commitment_weight=commitment_weight,
    )

    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    optimizer = AdamW(params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)

    # ===== W&B =====
    if wandb_logging and accelerator.is_main_process:
        wandb.login()
        run = wandb.init(
            project="rqvae-training-large-scale",
            config=params,
            name=f"large_L{vae_n_layers}_K{vae_codebook_size}_20M",
        )

    # ===== æ–­ç‚¹æ¢å¤ =====
    start_iter = 0
    if pretrained_rqvae_path:
        checkpoint = torch.load(pretrained_rqvae_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint["model"])
        optimizer.load_state_dict(checkpoint["optimizer"])
        start_iter = checkpoint["iter"] + 1
        print(f"ä»è¿­ä»£ {start_iter} ç»§ç»­è®­ç»ƒ")

    # Acceleratorå‡†å¤‡
    model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)
    if eval_dataloader is not None:
        eval_dataloader = accelerator.prepare(eval_dataloader)

    # ===== è®­ç»ƒå¾ªç¯ =====
    train_iter = iter(train_dataloader)
    losses = {"total": [], "recon": [], "vq": []}
    best_metrics = None

    os.makedirs(save_dir_root, exist_ok=True)

    with tqdm(initial=start_iter, total=iterations, disable=not accelerator.is_main_process) as pbar:
        for it in range(start_iter, iterations):
            model.train()

            # K-meansåˆå§‹åŒ–ï¼ˆé¦–æ¬¡ï¼‰
            if it == 0 and not pretrained_rqvae_path and vae_codebook_kmeans_init:
                print("\næ‰§è¡ŒK-meansåˆå§‹åŒ–...")
                model.eval()
                init_data = []
                for _ in range(20):  # å¤šæ”¶é›†ä¸€äº›æ•°æ®ç”¨äºåˆå§‹åŒ–
                    try:
                        batch = next(train_iter)
                    except StopIteration:
                        train_iter = iter(train_dataloader)
                        batch = next(train_iter)
                    init_data.append(batch)
                init_data = torch.cat(init_data, dim=0)
                print(f"K-meansåˆå§‹åŒ–æ•°æ®: {init_data.shape}")
                with torch.no_grad():
                    _ = model(init_data, gumbel_t=0.5)
                model.train()
                print("K-meansåˆå§‹åŒ–å®Œæˆ")

            # è®­ç»ƒæ­¥
            total_loss = 0.0
            optimizer.zero_grad(set_to_none=True)

            for _ in range(gradient_accumulate_every):
                try:
                    data = next(train_iter)
                except StopIteration:
                    train_iter = iter(train_dataloader)
                    data = next(train_iter)

                with accelerator.autocast():
                    out = model(data, gumbel_t=0.2)
                    loss = out.loss / gradient_accumulate_every
                    total_loss += loss

            accelerator.backward(total_loss)
            
            # æ¢¯åº¦è£å‰ªï¼ˆå¤§æ¨¡å‹è®­ç»ƒç¨³å®šæ€§ï¼‰
            if accelerator.sync_gradients:
                accelerator.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()

            # è®°å½•æŸå¤±
            losses["total"].append(total_loss.item() * gradient_accumulate_every)
            losses["recon"].append(out.reconstruction_loss.item())
            losses["vq"].append(out.rqvae_loss.item())
            for k in losses:
                losses[k] = losses[k][-1000:]

            # æ›´æ–°è¿›åº¦æ¡ & é‡‡æ ·æŒ‡æ ‡
            if it % 100 == 0:
                avg_loss = float(np.mean(losses["total"]))
                avg_recon = float(np.mean(losses["recon"]))
                avg_vq = float(np.mean(losses["vq"]))

                # è®°å½•è®­ç»ƒæŒ‡æ ‡
                metrics_history['steps'].append(it)
                metrics_history['total_loss'].append(avg_loss)
                metrics_history['recon_loss'].append(avg_recon)
                metrics_history['vq_loss'].append(avg_vq)

                # ä»æ¨¡å‹æœ€è¿‘æ—¥å¿—è§£æå„å±‚ä½¿ç”¨ä¸ ppl
                try:
                    recent_logs = model.module.util_recent_logs if hasattr(model, 'module') else getattr(model, 'util_recent_logs', [])
                except Exception:
                    recent_logs = []
                if recent_logs:
                    latest_log = recent_logs[-1]
                    for layer_idx in range(vae_n_layers):
                        pattern = f"L{layer_idx}:(\\d+)/\\d+ [\\d.]+% ppl\\(b\\)=([\\d.]+)"
                        match = re.search(pattern, latest_log)
                        if match:
                            usage = int(match.group(1))
                            ppl = float(match.group(2))
                            metrics_history['layer_usage'][layer_idx].append(usage)
                            metrics_history['layer_perplexity'][layer_idx].append(ppl)
                # å¯¹é½é•¿åº¦ï¼Œé¿å…ç»˜å›¾é”™ä½
                ensure_list_lengths(metrics_history)

                pbar.set_description(f"L:{avg_loss:.6f}|R:{avg_recon:.6f}|VQ:{avg_vq:.6f}")

            # W&Bè®°å½•
            if accelerator.is_main_process and wandb_logging and it % 100 == 0:
                log_dict = {
                    "train/loss": float(np.mean(losses["total"])),
                    "train/recon_loss": float(np.mean(losses["recon"])),
                    "train/vq_loss": float(np.mean(losses["vq"])),
                    "iter": it,
                }
                # æŸäº›å®ç°é‡Œ out å¯èƒ½æ²¡æœ‰ p_unique_idsï¼Œå®‰å…¨è·å–
                if hasattr(out, "p_unique_ids"):
                    try:
                        log_dict["train/p_unique_ids"] = out.p_unique_ids.item()
                    except Exception:
                        pass
                wandb.log(log_dict)

            # ======= è¯¦ç»†è¯„ä¼° =======
            if do_eval and (it + 1) % eval_every == 0 and eval_dataloader is not None:
                print(f"\n[è¯¦ç»†è¯„ä¼°å¼€å§‹] è¿­ä»£ {it}")
                
                # ä½¿ç”¨evaluatorè¿›è¡Œå…¨é¢è¯„ä¼°
                eval_metrics = evaluator.evaluate_model(
                    model=model,
                    eval_dataloader=eval_dataloader,
                    max_samples=eval_samples,
                    device=device
                )
                
                # æ›´æ–°å†å²è®°å½•
                evaluator.update_history(it, eval_metrics)
                
                # æ‰“å°è¯¦ç»†ç»“æœ
                print(f"[è¯„ä¼°ç»“æœ] é‡æ„æŸå¤±: {eval_metrics['eval_recon']:.6f}")
                print(f"[è¯„ä¼°ç»“æœ] æ€»æŸå¤±: {eval_metrics['eval_loss']:.6f}")
                print(f"[è¯„ä¼°ç»“æœ] æ ·æœ¬æ•°: {eval_metrics['samples_evaluated']:,}")
                
                # å„å±‚è¯¦æƒ…
                avg_usage = 0
                avg_dead = 0
                for layer_idx in range(vae_n_layers):
                    usage = eval_metrics[f'layer_{layer_idx}_usage_rate']
                    dead = eval_metrics[f'layer_{layer_idx}_dead_code_rate'] 
                    ppl = eval_metrics[f'layer_{layer_idx}_perplexity']
                    print(f"  Layer {layer_idx}: ä½¿ç”¨ç‡={usage:.3f} æ­»äº¡ç‡={dead:.3f} å›°æƒ‘åº¦={ppl:.1f}")
                    avg_usage += usage
                    avg_dead += dead
                
                avg_usage /= vae_n_layers
                avg_dead /= vae_n_layers
                
                print(f"[å¹³å‡æŒ‡æ ‡] ä½¿ç”¨ç‡: {avg_usage:.3f}, æ­»äº¡ç‡: {avg_dead:.3f}")
                print(f"[ç¢°æ’ç»Ÿè®¡] ç¢°æ’ç‡: {eval_metrics['collision_rate']:.3f}, æœ€å¤§æ¡¶: {eval_metrics['max_bucket_size']}")
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä¿å­˜ä¸ºbest
                if evaluator.should_save_as_best(eval_metrics, best_metrics):
                    best_metrics = eval_metrics.copy()
                    if accelerator.is_main_process:
                        os.makedirs(save_dir_root, exist_ok=True)
                        best_path = os.path.join(save_dir_root, "checkpoint_best.pt")
                        state = {
                            "iter": it,
                            "model": accelerator.get_state_dict(model),
                            "model_config": getattr(model, "config", {}),
                            "optimizer": optimizer.state_dict(),
                            "eval_metrics": eval_metrics,  # ä¿å­˜å®Œæ•´è¯„ä¼°æŒ‡æ ‡
                        }
                        torch.save(state, best_path)
                        
                        # ä¿å­˜è¯¦ç»†è¯„ä¼°æŠ¥å‘Š
                        eval_report_dir = os.path.join(save_dir_root, f"eval_report_step_{it}")
                        evaluator.save_evaluation_summary(eval_report_dir, eval_metrics)
                        
                        print(f"âœ“ æ–°çš„æœ€ä½³æ¨¡å‹! é‡æ„æŸå¤±: {eval_metrics['eval_recon']:.6f}")
                
                # æ£€æŸ¥æ”¶æ•›
                converged, conv_msg = evaluator.check_convergence(
                    min_steps=convergence_min_steps,
                    window_size=convergence_window_size,
                    usage_threshold=convergence_usage_threshold,
                    dead_code_threshold=convergence_dead_threshold,
                    stability_threshold=convergence_stability_threshold
                )
                
                if converged:
                    print(f"\nğŸ‰ æ¨¡å‹å·²æ”¶æ•›! {conv_msg}")
                    print(f"å¯ä»¥åœæ­¢è®­ç»ƒå¹¶å¯¼å‡ºæ¨¡å‹ç”¨äºç¬¬äºŒé˜¶æ®µã€‚")
                    
                    # å¯é€‰ï¼šè‡ªåŠ¨åœæ­¢è®­ç»ƒï¼ˆå¤§æ•°æ®é›†ä¸‹å»ºè®®æ‰‹åŠ¨æ§åˆ¶ï¼‰
                    # if it > convergence_min_steps + 5000:  # é¢å¤–è®­ç»ƒä¸€ç‚¹ç¡®ä¿ç¨³å®š
                    #     print("è‡ªåŠ¨åœæ­¢è®­ç»ƒ...")
                    #     break
                else:
                    print(f"â³ å°šæœªæ”¶æ•›: {conv_msg}")
                
                # W&Bè®°å½•è¯¦ç»†æŒ‡æ ‡
                if accelerator.is_main_process and wandb_logging:
                    # åŸºç¡€æŒ‡æ ‡
                    wandb_dict = {
                        "eval/recon_loss": eval_metrics['eval_recon'],
                        "eval/total_loss": eval_metrics['eval_loss'],
                        "eval/collision_rate": eval_metrics['collision_rate'],
                        "eval/max_bucket_size": eval_metrics['max_bucket_size'],
                        "eval/avg_usage_rate": avg_usage,
                        "eval/avg_dead_rate": avg_dead,
                        "iter": it,
                    }
                    
                    # å„å±‚æŒ‡æ ‡
                    for layer_idx in range(vae_n_layers):
                        prefix = f"eval/layer_{layer_idx}"
                        wandb_dict.update({
                            f"{prefix}/usage_rate": eval_metrics[f'layer_{layer_idx}_usage_rate'],
                            f"{prefix}/dead_code_rate": eval_metrics[f'layer_{layer_idx}_dead_code_rate'],
                            f"{prefix}/perplexity": eval_metrics[f'layer_{layer_idx}_perplexity'],
                            f"{prefix}/gini": eval_metrics[f'layer_{layer_idx}_gini'],
                        })
                    
                    wandb.log(wandb_dict)
                
                print()  # ç©ºè¡Œåˆ†éš”

            # ======= å®šæœŸä¿å­˜ =======
            if accelerator.is_main_process and (it + 1) % save_model_every == 0:
                os.makedirs(save_dir_root, exist_ok=True)
                save_path = os.path.join(save_dir_root, f"checkpoint_{it}.pt")
                state = {
                    "iter": it,
                    "model": accelerator.get_state_dict(model),
                    "model_config": getattr(model, "config", {}),
                    "optimizer": optimizer.state_dict(),
                }
                torch.save(state, save_path)

                # åˆ›å»º latest é“¾æ¥
                latest_path = os.path.join(save_dir_root, "checkpoint_latest.pt")
                if os.path.exists(latest_path):
                    try:
                        os.remove(latest_path)
                    except FileNotFoundError:
                        pass
                try:
                    os.symlink(f"checkpoint_{it}.pt", latest_path)
                except (FileExistsError, OSError):
                    # Windows ç³»ç»Ÿå¯èƒ½ä¸æ”¯æŒç¬¦å·é“¾æ¥ï¼Œç›´æ¥å¤åˆ¶
                    import shutil
                    shutil.copy2(save_path, latest_path)

                print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: checkpoint_{it}.pt")

            pbar.update(1)

    # ======= æœ€ç»ˆä¿å­˜ =======
    if accelerator.is_main_process:
        final_path = os.path.join(save_dir_root, "checkpoint_final.pt")
        state = {
            "iter": iterations - 1,
            "model": accelerator.get_state_dict(model),
            "model_config": getattr(model, "config", {}),
            "optimizer": optimizer.state_dict(),
        }
        os.makedirs(save_dir_root, exist_ok=True)
        torch.save(state, final_path)

        # ç”Ÿæˆå¯è§†åŒ–
        config_dict = {
            'vae_codebook_size': vae_codebook_size,
            'vae_n_layers': vae_n_layers,
            'commitment_weight': commitment_weight,
            'batch_size': batch_size,
            'learning_rate': learning_rate
        }
        save_training_visualizations(metrics_history, save_dir_root, config_dict)

        # ç”Ÿæˆæœ€ç»ˆè¯„ä¼°æŠ¥å‘Š
        if best_metrics is not None:
            final_report_dir = os.path.join(save_dir_root, "final_evaluation_report")
            evaluator.save_evaluation_summary(final_report_dir, best_metrics)
            
            print(f"\nğŸ“Š æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š:")
            print(f"  é‡æ„æŸå¤±: {best_metrics['eval_recon']:.6f}")
            avg_usage = np.mean([best_metrics[f'layer_{i}_usage_rate'] for i in range(vae_n_layers)])
            avg_dead = np.mean([best_metrics[f'layer_{i}_dead_code_rate'] for i in range(vae_n_layers)])
            print(f"  å¹³å‡ä½¿ç”¨ç‡: {avg_usage:.3f}")
            print(f"  å¹³å‡æ­»äº¡ç‡: {avg_dead:.3f}")
            print(f"  ç¢°æ’ç‡: {best_metrics['collision_rate']:.3f}")
            
            # åˆ¤æ–­æ˜¯å¦ready for stage 2
            quality_good = (
                avg_usage >= convergence_usage_threshold and 
                avg_dead <= convergence_dead_threshold and 
                best_metrics['collision_rate'] < 0.02  # ç¨å¾®æ”¾å®½åˆ°2%
            )
            
            if quality_good:
                print(f"âœ… æ¨¡å‹è´¨é‡è‰¯å¥½ï¼Œå»ºè®®ç”¨äºç¬¬äºŒé˜¶æ®µè®­ç»ƒï¼")
                print(f"ğŸ’¡ Semantic IDç”Ÿæˆè´¨é‡æŒ‡æ ‡:")
                print(f"   - å¹³å‡ä½¿ç”¨ç‡: {avg_usage:.1%} â‰¥ {convergence_usage_threshold:.1%} âœ“")
                print(f"   - å¹³å‡æ­»äº¡ç‡: {avg_dead:.1%} â‰¤ {convergence_dead_threshold:.1%} âœ“") 
                print(f"   - ç¢°æ’ç‡: {best_metrics['collision_rate']:.1%} < 2% âœ“")
            else:
                print(f"âš ï¸  æ¨¡å‹è´¨é‡éœ€è¦æ”¹è¿›ï¼š")
                if avg_usage < convergence_usage_threshold:
                    print(f"   - ä½¿ç”¨ç‡åä½: {avg_usage:.3f} < {convergence_usage_threshold}")
                    print(f"     å»ºè®®: é™ä½commitment_weight, å¢åŠ è®­ç»ƒæ—¶é—´")
                if avg_dead > convergence_dead_threshold:
                    print(f"   - æ­»äº¡ç è¿‡å¤š: {avg_dead:.3f} > {convergence_dead_threshold}")
                    print(f"     å»ºè®®: æ£€æŸ¥k-meansåˆå§‹åŒ–, è°ƒæ•´commitment_weight")
                if best_metrics['collision_rate'] >= 0.02:
                    print(f"   - ç¢°æ’ç‡è¿‡é«˜: {best_metrics['collision_rate']:.3f}")
                    print(f"     å»ºè®®: å¢åŠ codebook_sizeæˆ–å±‚æ•°")
        
        print(f"\nğŸ”§ ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®:")
        print(f"  1. æ£€æŸ¥æœ€ä½³æ¨¡å‹: {save_dir_root}/checkpoint_best.pt")
        print(f"  2. æŸ¥çœ‹è¯„ä¼°æŠ¥å‘Š: {save_dir_root}/final_evaluation_report/")
        print(f"  3. å¦‚è´¨é‡æ»¡æ„ï¼Œå¯ç”¨æ­¤æ¨¡å‹å¼€å§‹ç¬¬äºŒé˜¶æ®µ Seq2Seq è®­ç»ƒ")
        print(f"  4. å¤§æ•°æ®é›†å»ºè®®: å¯ä»¥é€‚å½“æ”¾å®½æ ‡å‡†ï¼Œ75%ä½¿ç”¨ç‡ä¹Ÿå¯æ¥å—")

        # ä¿å­˜æ¨¡å‹é…ç½®ä¾›ç¬¬äºŒé˜¶æ®µä½¿ç”¨
        model_config_path = os.path.join(save_dir_root, "model_config_for_stage2.json")
        
        # è½¬æ¢metricsä¸­çš„numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        clean_metrics = {}
        if best_metrics:
            for k, v in best_metrics.items():
                if isinstance(v, (np.integer, np.floating)):
                    clean_metrics[k] = float(v)
                elif isinstance(v, np.ndarray):
                    clean_metrics[k] = v.tolist()
                else:
                    clean_metrics[k] = v
        
        stage2_config = {
            "rqvae_config": {
                "input_dim": vae_input_dim,
                "embed_dim": vae_embed_dim,
                "hidden_dims": vae_hidden_dims,
                "codebook_size": vae_codebook_size,
                "n_layers": vae_n_layers,
                "commitment_weight": commitment_weight,
            },
            "semantic_id_length": vae_n_layers + 1,  # +1 for collision suffix
            "vocab_size_estimate": vae_codebook_size * vae_n_layers + 1000,  # +1000 for special tokens
            "best_checkpoint": "checkpoint_best.pt",
            "data_scale": "20M_vectors_5120dim",
            "quality_metrics": clean_metrics,  # ä½¿ç”¨æ¸…ç†åçš„metrics
        }
        
        with open(model_config_path, 'w') as f:
            json.dump(stage2_config, f, indent=2)
        print(f"  5. ç¬¬äºŒé˜¶æ®µé…ç½®å·²ä¿å­˜: {model_config_path}")

        print("\nRQ-VAEç¬¬ä¸€é˜¶æ®µè®­ç»ƒå®Œæˆï¼ğŸ‰")

    if wandb_logging:
        wandb.finish()


if __name__ == "__main__":
    parse_config()
    train()